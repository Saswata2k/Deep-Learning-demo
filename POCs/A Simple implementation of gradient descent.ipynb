{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "alphas = [0.001,0.01,0.1,1,10,100,1000]\n",
    "\n",
    "# compute sigmoid nonlinearity\n",
    "def sigmoid(x):\n",
    "    output = 1/(1+np.exp(-x))\n",
    "    return output\n",
    "\n",
    "# convert output of sigmoid function to its derivative\n",
    "def sigmoid_output_to_derivative(output):\n",
    "    return output*(1-output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array([[0,0,1],\n",
    "              [0,1,1],\n",
    "              [1,0,1],\n",
    "              [1,1,1]])\n",
    "                \n",
    "y = np.array([[0],\n",
    "              [1],\n",
    "              [1],\n",
    "              [0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training With Alpha: 0.001\n",
      "Error after 0 iterations:0.49641003190272537\n",
      "Error after 10000 iterations:0.49516402549338606\n",
      "Error after 20000 iterations:0.4935960431880486\n",
      "Error after 30000 iterations:0.4916063585594306\n",
      "Error after 40000 iterations:0.48910016654420474\n",
      "Error after 50000 iterations:0.48597785784615843\n",
      "\n",
      "Training With Alpha: 0.01\n",
      "Error after 0 iterations:0.49641003190272537\n",
      "Error after 10000 iterations:0.45743107444190134\n",
      "Error after 20000 iterations:0.359097202563399\n",
      "Error after 30000 iterations:0.23935813715897253\n",
      "Error after 40000 iterations:0.1430706590133703\n",
      "Error after 50000 iterations:0.09859642980892719\n",
      "\n",
      "Training With Alpha: 0.1\n",
      "Error after 0 iterations:0.49641003190272537\n",
      "Error after 10000 iterations:0.042888017000115755\n",
      "Error after 20000 iterations:0.02409899422852161\n",
      "Error after 30000 iterations:0.018110652146797843\n",
      "Error after 40000 iterations:0.014987616272210912\n",
      "Error after 50000 iterations:0.013014490538142586\n",
      "\n",
      "Training With Alpha: 1\n",
      "Error after 0 iterations:0.49641003190272537\n",
      "Error after 10000 iterations:0.008584525653247159\n",
      "Error after 20000 iterations:0.0057894598625078085\n",
      "Error after 30000 iterations:0.004629176776769985\n",
      "Error after 40000 iterations:0.003958765280273649\n",
      "Error after 50000 iterations:0.0035101225678616766\n",
      "\n",
      "Training With Alpha: 10\n",
      "Error after 0 iterations:0.49641003190272537\n",
      "Error after 10000 iterations:0.003129388763011837\n",
      "Error after 20000 iterations:0.002144595579852179\n",
      "Error after 30000 iterations:0.0017239754995622308\n",
      "Error after 40000 iterations:0.0014782145122908034\n",
      "Error after 50000 iterations:0.0013127406283356764\n",
      "\n",
      "Training With Alpha: 100\n",
      "Error after 0 iterations:0.49641003190272537\n",
      "Error after 10000 iterations:0.12547698383358538\n",
      "Error after 20000 iterations:0.12533033354043677\n",
      "Error after 30000 iterations:0.12526772879373652\n",
      "Error after 40000 iterations:0.12523107370012884\n",
      "Error after 50000 iterations:0.12520635280373757\n",
      "\n",
      "Training With Alpha: 1000\n",
      "Error after 0 iterations:0.49641003190272537\n",
      "Error after 10000 iterations:0.5\n",
      "Error after 20000 iterations:0.5\n",
      "Error after 30000 iterations:0.5\n",
      "Error after 40000 iterations:0.5\n",
      "Error after 50000 iterations:0.5\n"
     ]
    }
   ],
   "source": [
    "for alpha in alphas:\n",
    "    print (\"\\nTraining With Alpha: {}\".format(alpha))\n",
    "    np.random.seed(1)\n",
    "\n",
    "    # randomly initialize our weights with mean 0\n",
    "    synapse_0 = 2*np.random.random((3,4)) - 1\n",
    "    synapse_1 = 2*np.random.random((4,1)) - 1\n",
    "\n",
    "    for j in range(60000):\n",
    "\n",
    "        # Feed forward through layers 0, 1, and 2\n",
    "        layer_0 = X\n",
    "        layer_1 = sigmoid(np.dot(layer_0,synapse_0))\n",
    "        layer_2 = sigmoid(np.dot(layer_1,synapse_1))\n",
    "\n",
    "        # how much did we miss the target value?\n",
    "        layer_2_error = layer_2 - y\n",
    "\n",
    "        if (j% 10000) == 0:\n",
    "            print(\"Error after \"+str(j)+\" iterations:\" + str(np.mean(np.abs(layer_2_error))))\n",
    "\n",
    "        # in what direction is the target value?\n",
    "        # were we really sure? if so, don't change too much.\n",
    "        layer_2_delta = layer_2_error*sigmoid_output_to_derivative(layer_2)\n",
    "\n",
    "        # how much did each l1 value contribute to the l2 error (according to the weights)?\n",
    "        layer_1_error = layer_2_delta.dot(synapse_1.T)\n",
    "\n",
    "        # in what direction is the target l1?\n",
    "        # were we really sure? if so, don't change too much.\n",
    "        layer_1_delta = layer_1_error * sigmoid_output_to_derivative(layer_1)\n",
    "\n",
    "        synapse_1 -= alpha * (layer_1.T.dot(layer_2_delta))\n",
    "        synapse_0 -= alpha * (layer_0.T.dot(layer_1_delta))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Following are the findings: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alpha = 0.001 \n",
    "The network with a crazy small alpha didn't hardly converge! This is because we made the weight updates so small that they hardly changed anything, even after 60,000 iterations! This is textbook Problem 3:When Slopes Are Too Small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alpha = 1\n",
    "Interestingly, This had the exact convergence as if we had no alpha at all! Multiplying our weight updates by 1 doesn't change anything. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alpha = 10\n",
    "An alpha that was greater than 1 achieved the best score after only 10,000 iterations! This tells us that our weight updates were being too conservative with smaller alphas. This means that in the smaller alpha parameters (less than 10), the network's weights were generally headed in the right direction, they just needed to hurry up and get there!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alpha = 1000\n",
    "And with an extremely large alpha, we see an example of divergence, with the error increasing instead of decreasing... hardlining at 0.5. This is a more extreme version of Problem 3 where it overcorrectly whildly and ends up very far away from any local minimums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
